\documentclass{article}
\documentclass[utf8x, 12pt]{G7-32} % Стиль (по умолчанию будет 14pt)
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{mathtext}
\usepackage[numberright]{eskdplain}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{mathtext}
\usepackage[T1,T2A]{fontenc}
\usepackage[english,bulgarian,ukranian,russian]{babel}
\usepackage[argument]{graphicx}
\graphicspath{{pics/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}

\usepackage{geometry} % Меняем поля страницы
\geometry{left=2cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле

\begin{document}
    \begin{center} 
    \large МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ

федеральное государственное автономное образовательное учреждение высшего образования

Национальный исследовательский нижегородский государственный университет им. Н.И. Лобачевского

Институт информационных технологий, математики и механики \\
Кафедра математического обеспечения и суперкомпьютерных технологий\\[3.5cm] 
    
    \huge Отчёт по практике \\[0.6cm] % название работы, затем отступ 0,6см
    \\ 
    \huge{Тема :}\\[0.6cm]
    \huge Сети Generative adversarial networks для синтеза изображений по текстовому описанию\\[7.7cm]
    
    
    \end{center} 
    
    \begin{flushright}
    \large \textbf{Выполнил:} \\
    студент гр. 381706-1 \\
    Митягина Дарья Сергеевна \\
    \textbf{Научный руководитель:} \\
    Профессор, доктор технических наук\\
    Турлапов Вадим Евгеньевич \\
    [3.7cm]
    \end{flushright}
    
    % \vfill 
    
    \begin{center} 
    \large Нижний Новгород 2020
    \end{center} 
    
    \thispagestyle{empty}
    \newpage
      \begin{center}
    %   Содержание
        \tableofcontents
      \end{center}
      
    \newpage
    \begin{center} 
    \section{Введение}
    %   \huge \textbf{Введение} \\[1.3cm]
    \end{center} 
    
    \newpage
    \begin{center} 
    \section{Необходимый теоретический минимум (дополнение)}
    %   \huge \textbf{Введение} \\[1.3cm]
    \end{center} 
    \large 
    \textit{1. Глубокая сверточная сеть (DCGAN - Deep Convolutional Generative Adversarian Network)}
    \\
    Первое и главное улучшение в архитектуре GAN. Они более устойчивы для обучения и генерируют высокие по качеству результаты. Они также важны, поскольку стали одними из основных базовых показателей для внедрения и использования GAN. 
    \\ \\
    \textit{2. Условная сеть (Conditional GAN)}
    \\
    Эта сеть использует дополнительную информацию, а также есть возможность контролировать как будут выглядеть сгенерированные изображения. Условная GAN является расширением GAN. Мы имеем условную информацию Y, которая описывает некоторые аспекты информации. Например, если мы имеем дело с лицами, Y может описать описать такие атрибуты, как пол или цвет волос. Затем эту информацию помеща.т и в генератор и в дискриминатор. \\
    Интересным является то, что по мере того как вы обучаете модель и задаете ей информацию, GAN учится использовать ее и даже имеет возможность генерировать лучшие результаты.
    \\ \\
    \textit{3. Управляемая генеративно состязательная сеть (ControlGAN)}
    \\
    Новая архитектура генеративной модели для управления сгенерированными образцами называется управляемой генеративной состязательной сетью (ControlGAN). 
    \\
    ControlGAN состоит из трех частей: генератор / декодер, дискриминатор и классификатор / кодер. Генератор в ControlGAN соревнуется с дискриминатором и классификатором одновременно.
    Генератор стремится обмануть дискриминатор и правильно классифицировать его по классификатору. ControlGAN имеет два основных преимущества по сравнению с существующими моделями. 
    Во-первых, ControlGAN может быть обучен уделять больше внимания входным меткам, чтобы ControlGAN мог генерировать образцы с подробной функцией где условный GAN вряд ли может сгенерировать. Во-вторых, ControlGAN использует независимую сеть для сопоставление элементов в соответствующих входных метках. Следовательно, дискриминатор может больше сосредоточиться на своей собственной цели, которая заключается в различении поддельных образцов и оригинальных образцы, так что качество генерируемых образцов может быть улучшено.
    \\ \\
    \textit{4. Сверточно-рекуррентная нейронная сеть (CRNN)}
    \\
    Сверточные рекуррентные нейронные сети представляют собой комбинацию двух наиболее известных нейронных сетей. CRNN (сверточная рекуррентная нейронная сеть) включает CNN (сверточную нейронную сеть), за которой следует RNN (рекуррентные нейронные сети). Предложенная сеть аналогична CRNN, но дает лучшие или оптимальные результаты.
    \\
    Сеть начинается с традиционной двухмерной сверточной нейронной сети, за которой следуют пакетная нормализация, активация ELU, максимальный пул и выпадение с частотой выпадения 50 ${\%}$. Три таких сверточных слоя расположены последовательно с соответствующими активациями. За сверточными слоями следуют перестановка и слой изменения формы, который очень необходим для CRNN, поскольку форма вектора признаков отличается от CNN к RNN.
    \\ \\
    \textit{5. Нейронная сеть с прямой связью}
    \\
    Искусственная нейронная сеть, в которой соединения между узлами не образуют цикл. Такая сеть отличается от рекуррентной нейронной сети. Нейронная сеть с прямой связью была первым и самым простым типом искусственной нейронной сети. В этой сети информация перемещается только в одном направлении вперед от входных узлов, через скрытые узлы (если они есть) и к выходным узлам. В сети нет циклов или петель обратных связей.
    \\ \\
    \textit{6. Нормализация партии}
    \\
    Чтобы повысить стабильность нейронной сети, нормализация партии(batch normalization) нормализует выходные данные предыдущего слоя активации путем вычитания среднего значения партии и деления на стандартное отклонение партии. Пакетная нормализация позволяет каждому уровню сети учиться самостоятельно независимо от других уровней.
    \\ \\
    \textit{7. Партия (batch)}
    \\
    Что такое batch? Нельзя пропустить через нейронную сеть разом весь датасет. Поэтому делим данные на пакеты, сеты или партии, так же, как большая статья делится на много разделов — введение, градиентный спуск, эпохи, Batch size и итерации.
    \\ \\
    \textit{8. Leaky ReLU}
    \\
    Обычные ReLU не всегда достаточно надежны и в процессе обучения могут выходить из строя. Например, большой градиент, проходящий через ReLU, может привести к такому обновлению весов, что данный нейрон никогда больше не активируется. 
    ReLU с «утечкой» (leaky ReLU, LReLU) представляет собой одну из попыток решить эту проблему. Обычный ReLU на интервале x < 0 дает на выходе ноль, в то время как LReLU имеет на этом интервале небольшое отрицательное значение. То есть функция для LReLU имеет вид  f(x) = $\alpha$x при x < 0 и f(x) = x при x $\geqslant$ 0, где $\alpha$ – малая константа. Некоторые исследователи сообщают об успешном применении данной функции активации, но результаты не всегда стабильны.
    \\ \\
    \textit{9. GRU (Gated recurrent unit)}
    \\
    Управляемые рекуррентные блоки (Gated Recurrent Units, GRU) — механизм вентилей(gates) для рекуррентных нейронных сетей, представленный в 2014 году. 
    \\ \\
    \textit{10. Pooling}
    \\
    Слой POOL (слой пулинга) выполняет операцию по понижающей дискретизации пространственных размеров (ширина и высота), в результате чего объем может сократиться до [16×16×12]. То есть на этом этапе выполняется нелинейное уплотнение карты признаков. Логика работы такова: если на предыдущей операции свертки уже были выявлены некоторые признаки, то для дальнейшей обработки настолько подробное изображение уже не нужно, и оно уплотняется до менее подробной картинки.
    \\ \\
    \textit{11. }
    \newpage
    \begin{center} 
    \section{Краткий обзор источников}
    %   \huge \textbf{Статья ""} \\[1.3cm]
    \end{center}
    \large 
    В данной работе будут рассмотрены четыре статьи, две из которых будут разобраны более подробно.
    \\ \\
    \textit{1. Статья GAN синтез изображения из текста
    \\
    "Generative Adversarial Text to Image Synthesis", 
    \\
    Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee}
    \\

    В этой работе авторы задались целью перевести текст (человеческие письменные описания) непосредственно в пиксели изображения.
    \\
    Подход, предложенный авторами, заключается в обучении DC-GAN (Deep Convolutional Generative Adversarial Network), обусловленом текстовыми признаками, закодированными гибридной сверточно-рекуррентной нейронной сетью (CRNN) на уровне символов. Обе сети G (генератор) и сеть D (дискриминатор) выполняют вывод с прямой связью, обусловленный текстовым признаком.
    \\ \\
    \textit{2. Статья Text2Scene: создание композиционных сцен из текстовых описаний
    \\
    "Text2Scene: Generating Compositional Scenes from Textual Descriptions", 
    \\
    Fuwen Tan, Song Feng, Vicente Ordonez}
    \\

    В этой работе авторы представляют Text2Scene, модель для интерпретации визуально описательного языка для генерации композиции. Они специально сосредоточелись на создании представления сцены, состоящего из списка объектов вместе с их атрибутами (местоположение, размер, пропорции, внешний вид и т.д.). Описанный метод, в отличие от многих, не опирается на порождающие состязательные сети (GAN). Вместо этого создается интерпретируемая модель, которая итеративно генерирует сцену, предсказывая и добавляя новые объекты на каждом временном шаге.
    \\ \\
    \textit{3. Статья Создание соответствующего изображения из текстового описания, используя модифицированный алгоритм GAN-CLS
    \\
    "Generate the corresponding Image from Text Description using Modified GAN-CLS Algorithm", 
    \\
    Fuzhou Gonп and Zigeng Xia}
    \\

    Для оригинального GAN авторам было необходимо ввести случайный вектор с фиксированным распределением, а затем получить результирующий образец. Это означает, что не получается контролировать, какие образцы будет сеть генерировать напрямую, потому что мы не знаем соответствия между случайными векторами и выборками результатов. Поэтому условно предлагается conditional GAN (cGAN). Условие, которое может быть меткой класса или текстовое описание добавляется к входам как генератора, так и дискриминатора. В результате cGAN может генерировать образцы в соответствии с условием. 
    \\
    Алгоритм GAN-CLS основан на cGAN. У него есть еще один термин - целевая функция, которая содержит несовпадающие пары текстов и изображений. Авторы статьи показали доказательство модифицированного алгоритма GAN-CLS и их экспериментальный результат на наборе цветов Оксфорда-102 и CUB набор данных (dataset).
    \\ \\
    \textit{4. Статья Управляемое преобразование текста в изображение
    \\
    "Controllable Text-to-Image Generation",
    \\
    Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, Philip H. S. Torr}
    \\

    Целью данной статьи является создание изображений из текста, а также предоставление возможности пользователю манипулировать синтетическим изображением с использованием описания на естественном языке. В частности, авторы сосредоточились на изменении визуальныч атрибутов (категория, текстура, цвет и т.д.) объектов в сгенерированных изображениях путем изменения данного текстовое описание. 
    \\
    Чтобы достичь этого, был предложена новая controllable text-to-image generative adversarial network (ControlGAN), которая может синтезировать высококачественные изображения, а также позволяет пользователю может манипулировать атрибутами объектов, не влияя на генерацию другого контента. 
    \\
    Описанный ControlGAN содержит три новых компонента. Первый компонент - пространственный уровень и канальный генератор, управляемый вниманием, где механизм внимания используется, чтобы позволить генератору синтезировать субрегионы, соответствующие наиболее релевантным словам. Второй компонент - дискриминатор на уровне слов, где корреляция между словами и субрегионами изображений исследуется, чтобы распутать различные визуальные атрибуты. Третий компонент существует для того, чтобы заставить генератор сохранять визуальный внешний вид, связанный с неизмененным текстом
    
    \newpage
    \begin{center} 
    \section{Статья 1: GAN синтез изображения из текста}
    %   \huge \textbf{Статья ""} \\[1.3cm]
    \end{center} 
    \large 
    Основной целью авторов являлась разработка простой и эффективной архитектуры GAN и стратегии обучения, позволяющих получить убедительный синтез текста в изображение. В основном они использовали наборы данных Caltech-UCSD Birds и Oxford-102 вместе с пятью текстовыми описаниями для каждого изображения. Помимо птиц и цветов они применяли модель для более общих изображений и текстовых описаний в MS COCO датасете.
    \\
    Подход, изложенный в данной статье, заключается в обучении DC-GAN (Deep Convolutional Generative Adversarial Network). Обе сети G (генератор) и сеть D (дискриминатор) выполняют вывод с прямой связью, обусловленныйтекстовым признаком.
    \\ \\
    \textit{Архитектура сети:}
    \\
    Cеть генератор обозначается как G : $\mathbb{R}$ $^Z$ $\times$ $\mathbb{R}$ $^T$ $\rightarrow$ $\mathbb{R}$ $^D$, сеть дискриминатор 
    \\
    D : $\mathbb{R}$ $^D$ $\times$ $\mathbb{R}$ $^T$ $\rightarrow$ $\{$0, 1$\}$.
    Где Т - размерность текстового описания, D - размерность изображения, а Z - размерность шума на входе в G
    \\
    \begin{center} 
    \includegraphics[scale=0.55]{GAN.png}
    \\
    \caption{Рис. 1 - архитектура GAN}
    \\ \\
    \end{center} 
    В генераторе G сначала дискретизируется шум z $\in$ $\mathbb{R}$ $^Z$ $\sim$ N (0, 1), кодируется текстовый запрос t, используя текстовый кодировщик $\varphi$. Описание $\varphi$(t) сперва сжимается с использованием полностью связанного слоя до небольшого размера, за которым следует leaky-ReLU, а затем объединяется с вектором шума z. После этого вывод происходит как в обычной деконволюционной сети: мы передаем его через генератор G; синтетическое изображение $\hat{x}$ генерируется через $\hat{x}$ $\leftarrow$ G(z, $\varphi$(t)). Генерация изображения соответствует выводу прямой связи в генераторе G,  обусловленого текстом запроса и образцом шума. В дискриминаторе D выполняются несколько слоев свертки, затем следует leaky-ReLU. Затем выполняется свертка 1 × 1 с последующей сверткой 4 × 4 чтобы вычислить окончательный результат из D. 
    Нормализация партии(batch) выполняется на всех сверточных слоях.
    \\ \\
    Алгоритм обучения GAN-CLS, взятый из статьи :
    \\
    1:   \textbf{Input}: minibatch images x, matching text t, mismatching $\hat{t}$, number of training batch steps S\\
    2:   \textbf{for} n = 1 \textbf{to} S \textbf{do}\\
    3:\indent   h $\leftarrow$ ϕ(t) $\{$Encode matching text description$\}$\\
    4:\indent   $\hat{h}$ $\leftarrow$ ϕ($\hat{t}$) {Encode mis-matching text description}\\
    5:\indent   z $\sim$ N (0, 1)Z $\{$Draw sample of random noise$\}$\\
    6:\indent   $\hat{x}$ ← G(z, h) $\{$Forward through generator$\}$\\
    7:\indent   $s_{r}$ $\leftarrow$ D(x, h) $\{$real image, right text$\}$\\
    8:\indent   $s_{w}$ $\leftarrow$ D(x, $\hat{h}$) $\{$real image, wrong text$\}$\\
    9:\indent   $s_{f}$ $\leftarrow$ D($\hat{x}$, h) $\{$fake image, right text$\}$\\
    10:\indent $L_{D}$ $\leftarrow$ log($s_{r}$) + (log(1 $- s_{w}$) + log(1$ - s_{f}$ ))/2\\
    11:\indent D $\leftarrow$ D $- \alpha \delta$ $L_{D}$ / $\delta$D $\{$Update discriminator$\}$\\
    12:\indent $L_{G}$ $\leftarrow$ log($s_{f}$)\\
    13:\indent G $\leftarrow$ G $- \alpha \delta$ $L_{G}$/$\delta$ G $\{$Update generator$\}$\\
    14:  \textbf {$\{$end for$\}$}\\
    \\
    \textit{Рассмотрим алгоритм более подробно.} \\
    После кодирования текста, изображения и шума (строки 3-5) генерируется поддельное изображение ($\hat{x}$, строка 6). $s_{r}$ указывает оценку связывания реального изображения и соответствующего предложения (строка 7), $s_{w}$ измеряет балл ассоциирования реального изображения с произвольным предложением (строка 8), а $s_{f}$ - балл ассоциирования поддельное изображение с соответствующим текстом (строка 9).
    \\ \\
    \textit{Осуществление передачи стиля.}\\
    Если кодировка текста $\varphi$(t) захватывает содержание изображения (форма и цвета объектовов), то для получения реалистичного изображения образец шума z должен отражать факторы стиля такие как цвет фона и так далее. С обученным GAN, можно пожелать перенести стиль изображения на содержание конкретного текстового описания. Для этого можно обучить сверточную сеть, чтобы инвертировать G, чтобы регрессировать из образцов $\hat{x}$ $\leftarrow$ G (z, $\varphi$(t)) обратно на z. Авторы использовали простое возведение в квадрат для обучения кодировщика стиля:\\
    $L_{style}$ = $E_{t,z \sim N(0,1)}$||z − S(G(z, $\varphi$(t)))||$_{2}^2$\\
    где S - сеть кодировщика стиля. Перенос стиля из изображения x запроса в текст t происходит следующим образом:\\
    s $\leftarrow$ S(x), $\hat{x}$ $\leftarrow$ G(s, ϕ(t))\\
    Где $\hat{x}$ является результирующим изображением, а s является предсказанным стилем.

    \newpage
    \begin{center} 
    \section{Статья 2: Создание соответствующего изображения из текстового описания, используя модифицированный алгоритм GAN-CLS}
    %   \huge \textbf{Статья ""} \\[1.3cm]
    \end{center} 
    \large 
    Алгоритм GAN-CLS устанавливается на основе cGAN (Conditional GAN), а целевая функция модифицируется для того, чтобы дискриминатор мог судить, является ли входной текст и изображение соответствующими.\\
    Целевая функция этого алгоритма:\\ \\
    $\underset{G}{min}$ $\underset{D}{max}$E$_{(x,h) \sim p_{d}(x,h)}$ [logD(x, h)] + $^1/_2$ E$_{z \sim p_{z}(z),h \sim p_{d}(h)}$[log(1 - D(G(z, h)), h)]+$^1/_2$ E$_{z(x,h) \sim p_{\hat{d}}(x,h)}$[log(1 - D(x, h))].
    \\ \\
    Где h - вложение текста. $p_{d}$(x, h) - это распределение функция плотности выборок из набора данных, в которой совпадают x и h. $p_{\hat{d}}$(x, h) - функция плотности распределения выборок из набора данных, состоящего из текста и несоответствующего изображения.
    \\
    \begin{center} 
    \includegraphics[scale=0.55]{second.png}
    \\
    \caption{Рис. 1 - структура сети}
    \\ \\
    \end{center} 
    Дискриминатор имеет 3 вида входных данных:\\
    сопоставленные пары изображений и текста (x, h) из набора данных, текст и неправильное изображение ($\hat{x}$, h) из набор данных, текст и соответствующее сгенерированное изображение (G (z, h), h).
    \\ \\
    \textit{Модифицированный алгоритм:}\\
    Метод заключается в том, что мы модифицируем целевую функцию алгоритма.
    \\
    \\
    Требуется: minibatch size m; learning rate $\epsilon$; number of iterations N; dataset X
    1: \textbf {for} i in 1 to N \textbf {do}\\
    2: \indent extract m samples $\{$ (x$^(1)$, t$^(1)$),(x$^(2)$, t$^(2)$), ...,(x$^(m)$, t$^(m)$)$\}$ from one class of the dataset X, where x$^(i)$ is the image and t$^(i)$ is the corresponding text description.\\
    3: \indent extract m images $\{$ $\hat{x}^(1)$, $\hat{x}^(2)$, ..., $\hat{x}^(m)$ $\}$ from another class in X.\\
    4: \indent encode the text descriptions: h$^(i)$ = $\varphi$(t$^(i)$), i = 1, ..., m.\\
    5: \indent extract m random vectors {z$^(1)$, z$^(2)$, ..., z$^(m)$} from the distribution p$_z$(z).\\
    6: \indent generate images $\widetilde{x}^(i)$ = G(z(i), h(i)), i = 1, ..., m.\\
    7: \indent calculate LD = −1m Pm i=112[log(D(x(i), h(i))) + log(1 − D(˜x(i), h(i))) + log(D(ˆx(i), h(i))) + log(1 − D(ˆx(i), h(i)))].\\
    8: \indent update the parameters of the discriminat or: θd ← θd − ∇θd LD(θd).\\
    9: \indent calculate LG = 1 m Pm i=112log(1 − D(˜x( i), h(i)))).\\
    10: \indent update the parameters of the generator: θg ← θg − ∇θg LG(θg).\\
    11: \textbf {end for}
    
    \newpage
    \begin{center} 
    \section{Практика}
    %   \huge \textbf{Практика} \\[1.3cm]
    \end{center} 
    
    \newpage
    \begin{center} 
    \section{Заключение}
    %   \huge \textbf{Заключение} \\[1.3cm]
    \end{center} 
    
    \newpage
    \begin{center} 
    \section{Список литературы}
    %   \huge \textbf{Список литературы} \\[1.3cm]
    \end{center} 
    
\end{document}
